include "application.base.conf"

# this variables have to be replaced
# * SERVICE_IP
# ** IP adresse service should bind to, e.g. 127.0.0.1 or 0.0.0.0
# # * SERVICE_PORT
# ** port which service should use, e.g. 8080
# * ENVID
# ** environment id (e.g. ubirch-demo, ubirch-prod, ...)
# * CLUSTER_NAME
# ** name of the used elasticsearch cluster, default is elasticsearch
# * ES_HOST
# ** full hostname of ElastisSearch 5.3 cluster (e.g.: 1234asdf.eu-west-1.aws.found.io:9343)
# * ES_CLUSTER_NAME
# ** Elasticsearch cluster to connect to
# * ELASTIC_IO_USER
# ** user to connect to cloud.elastic.co with
# * ELASTIC_IO_PASSWORD
# ** password to connect to cloud.elastic.co with
# * GO_PIPELINE_NAME_AVATAR
# * GO_PIPELINE_LABEL_AVATAR
# * GO_REVISION_AVATAR
# * ENVID
# ** short string which will used to seperate different deployment enviroments, e.g. ubdev, ubmwc, trdev
# ** have to be unique for each deployment enviroment
# * MQTT_USER
# ** MQTT server username
# * MQTT_PASSWORD
# ** MQTT server password
# * MQTT_URL
# ** MQTT server url
# * MONGO_USER
# ** user name required to access MongoDB
# * MONGO_PASSWORD
# ** password required to access MongoDB
# * MONGO_OPTIONS
# ** MongoDB connection options
# * MONGO_HOST_1
# ** MongoDB url of host_1
# * ES_LARGE_PAGE_SIZE
# ** configures maximum size of some Elasticsearch queries
# * KAFKA_PROD_DELIVERY_TIMEOUT_MS
# ** timeout of producing message to Kafka
# * KAFKA_PROD_LINGERMS
# ** linger time of producing message to Kafka
# * KAFKA_PROD_BOOTSTRAP
# ** bootstrap server url of Kafka producer
# * KAFKA_TRACKLE_MSGPACK_TOPIC
# ** topic name for msgpack

akka {
  loglevel = "DEBUG"
  http {
    server {
      max-connections = 4096
      pipelining-limit = 64
      backlog = 100
    }
    host-connection-pool {
      max-connections = 512
      min-connections = 32
      max-open-requests = 128
    }
  }
  kafka {
    producer {
      kafka-clients {
        acks = 1
        delivery.timeout.ms = 120000 // default(2 minutes)
        delivery.timeout.ms = ${?KAFKA_PROD_DELIVERY_TIMEOUT_MS} // ms
        linger.ms = 0 // default
        linger.ms = ${?KAFKA_PROD_LINGERMS} // ms
      }
    }
  }
}

rediscala {
  loglevel = "DEBUG"
  rediscala-client-worker-dispatcher {
    mailbox-type = "akka.dispatch.SingleConsumerOnlyUnboundedMailbox"
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 512
  }
}

ubirchAvatarService {

  interface = "0.0.0.0"
  port = 8080

  prometheus {
    interface = "0.0.0.0"
    port = 8081
  }

  udp {
    interface = 0.0.0.0
    port = 9090
  }

  akka {
    actorTimeout = ${AKKA_ACTOR_TIMEOUT} // seconds
    numberOfFrontendWorkers = 15
    numberOfBackendWorkers = 8
  }

  mongo {
    reactiveMongoOptions = ""
    reactiveMongoOptions = ${?REACTIVE_MONGO_OPTIONS}
    hosts = ${MONGO_URI}${ubirchAvatarService.mongo.reactiveMongoOptions}
  }

  mqtt {
    broker {
      //      url = "tcp://localhost:1883"
      url = ${MQTT_URL}
    }
    queues {
      deviceBaseTopic = ${ENVID}"/ubirch/devices"
      devicesTopicPartin = "in"
      devicesTopicPartout = "out"
    }
    qos = 1
    publishProcessed = false
    publishProcessed = ${?MQTT_PUBLISH_PROCESSED}
  }

  es {
    defaultPageSize = ${?ES_DEFAULT_PAGE_SIZE} #10
    largePageSize = ${?ES_LARGE_PAGE_SIZE}
    device.index = ${?ES_DEVICE_INDEX} #"ubirch-devices"
    devicerawdata.index = ${?ES_DEVICE_RAW_DATA_INDEX} #"ubirch-device-raw-data"
    devicerawdataAnchored.index = ${?ES_DEVICE_RAW_DATA_ANCHORED_INDEX} #"ubirch-device-raw-data-anchored"
    devicehistory.index = ${?ES_DEVICE_HISTORY_INDEX} #"ubirch-device-history"
    devicetype.index = ${?ES_DEVICE_TYPE_INDEX} #"ubirch-device-type"
    devicestate.index = ${?ES_DEVICE_STATE_INDEX} #"ubirch-device-state"
  }

  kafka {
    producer {
      bootstrapServers = ${KAFKA_PROD_BOOTSTRAP}
    }
    trackleMsgpackTopic = ${KAFKA_TRACKLE_MSGPACK_TOPIC}
  }
}

esHighLevelClient {
  connection {
    host = ${ES_HOST}
    port = ${ES_PORT}
    scheme = ${ES_SCHEME}
    user = ${ELASTIC_IO_USER}
    password = ${ELASTIC_IO_PASSWORD}
    maxRetries = ${ES_MAX_NUMBER_OF_RETRIES}
    retry_delay_factor = ${ES_DELAY_FACTOR_IN_SECONDS}
    connectionTimeout = ${ES_CONNECTION_TIMEOUT}
    socketTimeout = ${ES_SOCKET_TIMEOUT}
    connectionRequestTimeout = ${ES_CONNECTION_REQUEST_TIMEOUT}
  }
  bulk {
    bulkActions = ${ES_CLIENT_BULK_ACTIONS}
    bulkSize = ${ES_CLIENT_BULK_SIZE} # bulkSize in mega bytes
    flushInterval = ${ES_CLIENT_BULK_FLUSH} # flush every x seconds
    concurrentRequests = ${ES_CLIENT_CONCURRENCY} # connection pooling: max concurrent requests
  }
}

ubirchAvatarService.client.rest.host = ${AVATAR_BASE_URL}
ubirchUserService.client.rest.host = ${UBIRCH_USERSERVICE_URL}
ubirchIdService.client {
  rest.host = ${ID_BASE_URL}
  redis.cache.maxTTL = ${ID_SERVICE_CACHE_MAX_TTL}
}


ubirch {

  envid = ${ENVID}

  oidcUtils {

    skipEnvChecking = ${OIDC_UTILS_SKIP_ENV_CHECKING}
    allowInvalidSignature = ${OIDC_UTILS_ALLOW_INVALID_SIGNATURE}
    # token age in min
    maxTokenAge = ${OIDC_UTILS_MAX_TOKEN_AGE_MINUTES}
    skipTokenAgeCheck = false

    redis {
      #duration of userContext cache expiration
      updateExpirySeconds = ${OIDC_UTILS_REDIS_UPDATE_EXPIRY_SECONDS}
    }
  }

  redisUtil {
    host = ${REDIS_HOST}
    port = ${REDIS_PORT}
    password = ${?REDIS_PASSWORD}
  }

}

crypto {
  ecc {
    // ed25519-sha-512 private key
    signingPrivateKey = ${SIGNING_PRIVATE_KEY}
  }
}
